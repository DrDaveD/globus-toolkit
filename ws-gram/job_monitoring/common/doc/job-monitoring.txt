Relevant statements from the requirements document:

1. Requirements
  a. Improve performance
    7.  Provide timely notifications when state changes occur in a job
        There is a noticeable delay between when a job state changes and when
        GRAM sends out notification of the state change.

...
  e. Add New Features
    3.  Make the history of all job states for a given managed job available
    to the client
        A requirement from a client point of view is to be able to trace the
        last n GRAM states in which it were. Since the number of job states in
        the GRAM model is finite and small, and since the job state machine is
        linear and acyclic, we propose to have the job keep a full history of
        its previous states.
    4.  Make the exit code of a job available to clients
        A requirement from a client perspective is to be able to query the
        exit code returned by the application executed by the job.

... 

2. Design Approaches
...
  8. Optimize interface to schedulers (Req. 1.a.7)
    The periodic execution by RIPS of the queue inspection command(s) of the
    back-end scheduler adds a significant overhead to the architecture. RIPS
    keeps tracks of previous job states and compare each polled results with
    the previous state of the queue so as to infer the new states of the jobs.
    This is causing serious scalability issues. 
    
    We want to remove RIPS and replace it with a scheduler-specific push
    model.  Ideally the scheduler (or a GRAM-specific plug-in to the
    scheduler) would notify the job manager, for instance through email
    notifications.  Unfortunately not all schedulers offer notifications. For
    instance Platform LSF does not support any form of push model, however it
    seems feasible to write a command executed by LSF to monitor a LSF log
    file periodically. While still a polling type of monitoring, this is much
    better than comparing new and old queue information as RIPS does. 

3) Immediate implementation tasks
...
    b.Work out scheduler adapter interface (Joe)
        i.Forked case
            1.Get rid of polling
                a.Does java have a way to get notified on process completion
        ii.Scheduler adapters
            1.monitoring
                a.parse the log file
                b.communication path btw log scanner and java code
                    i.bootstrapping process

From Karl's Mail
    6) eventd crawls over logfile in schedd account and pushes notifications
    into RIPS2.  RIPS2 is a simple demuxer, rather than the complicated
    soft-state comparator that it is in GT3.  RIPS2 knows the local
    jobID-->GRAM job EPR mapping.

and
    However, if it is only to avoid the notification race, it
    might be more efficient to just remember the last job state for all local
    jobs based on some reasonable soft-state expiry model... as soon as the
    local/gram mapping is known and recorded wherever needed for event
    routing, a query can be made for the most recent known state of that local
    ID (in case it arrived before the routing information was in place).  The
    reason for the soft-state is to avoid tracking non-gram jobs forever...

So, to summarize
What we dislike about RIPS
- Polls scheduler with queue status command in a provider which
  causes high load
- keeps track of previous state of all jobs between polls to determine
  when transitions occur
    - takes two poll cycles to determine what state a job is in
    - uses too much memory
- queue polling commands often do not show job exit code and in some cases,
  successful vs unsuccessful terminate

What we really want is something that:
- sends us job state changes within XXX seconds of the job state change
  occurring
- tells us when jobs enter the scheduler's queue, start execution, abort,
  or terminate
- provides a job exit code if scheduler provides it

Issues to resolve:
- How does the monitoring part of the system obtain information about jobs?
  - Parse scheduler status output [poor scalability]
    - GT2 tried this (one per job at interval), scaled horribly
    - GT3 tried this (one per host at interval), scaled horribly
  - Parse email notifications from schedulers
    - Very event driven 
    - Potential deployment issues:
      - can globus daemon receive mail on head node
      - can globus daemon have a pipe in its .forward file 
    - reliability issues
      - email is insecure (easy to forge notifications)
      - email can get queued for long period of time
      - spool filesystem could fill up
  - Parse scheduler log files? [Stu likes this]
    - Deployment issues
      - Is location of log files easy to determine (setup package hacks?)
      - How can we handle log file rotation
        - track inode of log we're currently parsing... check for rotation
          by comparing inode of filename of log with what we were reading?
      - Can admins decide what is logged or log format (to confuse our
        parsers)
      - Are log file formats similar enough for various POSIX 1003.2d
        (qsub-like) scheduler implementations?
    - Reliability issues
      - How do we restart service after a service restart such that we
        don't lose events and potentially job state changes
        - Can we fall back to a scheduler polling model in this case?
  - Fork is entirely different

- How do job state changes go from monitor to job manager service?
  - In GT 3.X, the Managed Job Service subscribed to service data
    notifications for specific job IDs to the RIPS service in a separate
    hosting environment.
  - Do we want to retain a subscription-type model in GT 4.0?
    - at what granularity do we want to subscribe

- Questions:
  What about the other part of RIPS (scheduler queue information, etc).
