#ifndef GLOBUS_DONT_DOCUMENT_INTERNAL
/** 
 * @mainpage Globus I/O
 *
 * <h2>Open Issues in the Globus I/O Library</h2>
 * <dl>
 * <dt>Performance of blocking I/O calls
 * <dd>
 * The function calls globus_io_write() and globus_io_read() are
 * calls which block until the I/O operation completes. In Globus
 * 1.1.0-1.1.2, these were implemented as calls to the nonblocking
 * functions globus_io_register_write() and
 * globus_io_register_read(). This requires all blocking I/O
 * operations to first be put into the select mask, selected as
 * active, and then a non-blocking I/O operation would be
 * attempted. In the threaded case, this included a thread switch to
 * the I/O event handling thread. Experimental results indicate that
 * the Globus I/O calls have a 40usec (80usec with threads) latency
 * penalty compared blocking socket API calls (on anl dg cluster).
 *
 * There exist calls globus_io_try_read() and globus_io_try_write()
 * which will attempt a non-blocking I/O operation without first doing
 * a select on the descriptor. Inserting a call to the appropriate
 * function into globus_io_read and globus_io_write helps if the
 * descriptor is ready for I/O right away.
 * </dd>
 * <dt>I/O starvation
 * <dd>
 * The Globus I/O core event loop dispatches I/O callbacks unfairly In
 * Globus 1.1.0-1.1.2, the process of dispatching a Globus I/O
 * callback would always set a flag which caused Globus I/O to exit
 * the event loop after one handle's I/O callback was dispatched. The
 * combination of this, and the event loop always testing descriptors
 * starting at fd "0", caused lower-numbered descriptors to be
 * serviced much more frequently than higher-numbered
 * descriptors. Additionally, there is an overhead of one select per
 * I/O callback, even when the select indicates that multiple
 * decsriptors are ready for I/O.
 *
 * The core loop doesn't need to re-select after a descriptor is
 * unregistered. There are two fd_sets associated with each select
 * operation, a currently-registered one (globus_l_io_read_fds, etc),
 * and the one resulting from the select operation
 * (globus_l_io_active_read_fds, etc).  Instead of giving up on the
 * result fd masks after registration information has changed, we can
 * check to make sure that the descriptor that we are going to
 * callback on is in both sets.
 *
 * We don't currently have a Globus I/O contention test to see if this
 * helps.
 * </dd>
 * <dt>Use of the memory allocator in asynchronous Globus I/O calls
 * <dd>
 * The combination of malloc and free showed up in a execution profile
 * as taking up the most time of any non-I/O operation (except for
 * gettimeofday).  About 1/2 of the malloc/free operations occurred in
 * globus_io registration/callback dispatch. (For secure connections
 * this would be higher).
 *
 * I think that the select_info structs in the fd array should be
 * modified to contain whatever information is currently malloc()ed
 * per registration.  I've added a few hacks over the years to make
 * sure that the mallocs aren't leaky; I think that some of these may
 * be removed if all this is done correctly.
 *</dd>
 * <dt>Select FD upper bound
 * <dd>
 * Using select with large descriptor sets can be slow. Both the
 * kernel and the event loop need to search through the fd_set from 0
 * to the maximum passed to select(). We keep a global variable which
 * contains the highest fd ever used by Globus I/O; however, we never
 * decrement this when the highest FD is closed.
 * </dd>
 * <dt>Alternatives to select()?
 * <dd>
 * Should we investigate alternative implementations of Globus I/O?
 * For example, the POSIX.4 aio_*() calls (linux, and irix support
 * them natively) provide threaded callbacks on I/O completion. (The
 * Globus I/O API is very close to the POSIX aio calls).
 *
 * Alternatively, we could investigate poll(2), in which you provide
 * an array of pollfd structs, so the kernel only has to poll
 * descriptors you are interested in, instead of all descriptors less
 * than the max. Since we are already allocating the fd sets, it may
 * not be too much of a stretch to dynamically allocate struct
 * pollfds. (In fact, using poll, we wouldn't have to do a memcpy of
 * the fd_sets before each select, as the pollfd struct has both an
 * input and an output event mask.)
 *
 * Yet another alternative is to have a pool of threads doing select
 * or I/O calls. This could help when the number of fds is large, or
 * the level of I/O activity is very high.
 *
 * Implementing another "core" of globus_io may help identify changes
 * in the globus_io code structure which would make a port to NT using
 * completion ports easier to do.
 * </dd>
 * <dt>Globus I/O locks
 * <dd>
 * A single lock is used in Globus I/O to deal with all concurrency
 * issues.  In some places it used use to protect the discriptor masks
 * for select, in others, to protect I/O handle data structures, and
 * in others to protect a global GSS Credential. (Aside: Should the
 * GSSAPI be thread-safe?)
 * </dd>
 *
 * <dt>Globus I/O cancel is broken
 * <dd>
 * This is not so much a performance issue as an API design issue. The
 * globus_io_cancel() API function is defined as cancelling whatever
 * happens to be registered at the time, optionally calling pending
 * callbacks.  There is no guarantee that any operation will be
 * cancelled. Cancellation should be somehow tied to actual
 * registration calls.
 * </dd>
 * <dt>Delegation code isn't quite there
 * <dd>
 * The changes that Doug made to the delegation in order to bypass the
 * environment variable setting haven't been propogated into Globus
 * I/O yet.
 * </dd>
 * <dt>Security changes to Globus I/O authorization (Globus Req #1891)
 * <dd>
 * Add support for GLOBUS_IO_SECURE_AUTHORIZATION_MODE_CALLBACK on the
 * client (connect) side
 *
 * add a GLOBUS_IO_SECURE_AUTHORIZATION_MODE_HOST to authorize a
 * particular host on the client (connect) side.
 * </dd>
 * <dt>Error reporting
 * <dd>
 * Globus I/O is the first bit of Globus to use the error/object
 * system. Due to some miscommunications and time pressures, the
 * Globus I/O API passes globus_result_t data types to callback
 * functions, instead of globus_object_t data, which was the intended
 * usage. Also, Globus I/O uses it's own error hierarchy, which is
 * different from the one defined in globus_common(should these be
 * combined?).
 * </dd>
 * <dt>Large files
 * <dd>
 * Globus I/O and Globus configure do not attempt to check to see if
 * 64 bit file pointers are supported, nor provide a way for users to
 * determine if they are supported (short of
 * sizeof(globus_size_t)). Globus I/O defines it's offset type
 * (globus_io_off_t) instead of it being defined in globus_libc.h
 * (where globus_size_t is defined).
 * </dd>
 * <dt>Defer to external event driver</dt>
 * <dd>
 * Globus I/O still does not have any way to play well with another event
 * driver.
 * </dd>
 */
#endif
